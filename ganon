#!/usr/bin/python3

import argparse, os, sys, subprocess, io, time, shlex, shutil, gzip, pickle, math
from collections import defaultdict

def main():
    version = '1.0'

    ####################################################################################################

    build_parser = argparse.ArgumentParser(description='Build options', add_help=False)
    
    # Required
    build_group_required = build_parser.add_argument_group('required arguments')
    build_group_required.add_argument('-d', '--db-prefix',      required=True, type=str,                    metavar='db_prefix',        help='Database output prefix (.filter, .nodes, .bins. .map will be created)')
    build_group_required.add_argument('-i', '--input-files',    required=True, type=str, nargs="*",         metavar='refs.fasta[.gz]',  help='Multi-fasta[.gz] file[s]')

    # Defaults
    build_group_optional = build_parser.add_argument_group('optional arguments')
    build_group_optional.add_argument('-k', '--kmer-size',                      type=int,   default=19,     metavar='', help='The k-mer size for the bloom filter [14..32]. Default: 19')
    build_group_optional.add_argument('-n', '--hash-functions',                 type=int,   default=3,      metavar='', help='The number of hash functions to use for the bloom filter [2..5]. Default: 3')
    build_group_optional.add_argument('-f', '--max-fp',                         type=float, default=0.05,   metavar='', help='Max. false positive rate for k-mer classification. Filter size will be defined base on this value by the estimated number of unique k-mers [Mutually exclusive --bloom-size]. Default: 0.05')
    build_group_optional.add_argument('-s', '--bloom-size',                     type=int,                   metavar='', help='Fixed size for filter in Megabytes (MB) [Mutually exclusive --min-fp] ')
    build_group_optional.add_argument('-t', '--threads',                        type=int,   default=2,      metavar='', help='set the number of subprocesses/threads to use for calculations. Default: 2')
    build_group_optional.add_argument('-r', '--rank',                           type=str,   default='taxid', metavar='', help='Lowest taxonomic rank for classification [assembly,taxid,species,genus,...]. Default: taxid')
    build_group_optional.add_argument('-l', '--bin-length',                     type=int,   default=6000000, metavar='', help='Maximum length (in bp) for each bin. Default: 6000000')
    build_group_optional.add_argument('-a', '--fragment-length',                type=int,   default=-1,      metavar='', help='Fragment length (in bp). Set to 0 to not fragment sequences. (default: bin-length - overlap-length)')
    build_group_optional.add_argument('-o', '--overlap-length',                 type=int,   default=500,     metavar='', help='Fragment overlap length (in bp). Should be bigger than the read length used for classificaiton. Default: 500')
    build_group_optional.add_argument('--len-taxid-file',                       type=str,                   metavar='', help='Tab-separated file with the sequence ids, sequence length, taxonomic id and optionally the assembly accession in case of assembly level classification is chosen')
    build_group_optional.add_argument('--nodes-file',                           type=str,                   metavar='', help='nodes.dmp from NCBI Taxonomy')
    build_group_optional.add_argument('--merged-file',                          type=str,                   metavar='', help='merged.dmp from NCBI Taxonomy')
    build_group_optional.add_argument('--names-file',                           type=str,                   metavar='', help='names.dmp from NCBI Taxonomy')
    # Extra
    build_group_optional.add_argument('--verbose',                                          default=False, action='store_true', help='Verbose mode for ganon')
    build_group_optional.add_argument('--ganon-path', type=str, default="./", help=argparse.SUPPRESS)
    build_group_optional.add_argument('--taxsbp-path', type=str, default="./", help=argparse.SUPPRESS)
    build_group_optional.add_argument('--n-refs', type=int, help=argparse.SUPPRESS)
    build_group_optional.add_argument('--n-batches', type=int, help=argparse.SUPPRESS)

    ####################################################################################################

    update_parser = argparse.ArgumentParser(description='Update options', add_help=False)

    # Required
    update_group_required = update_parser.add_argument_group('required arguments')
    update_group_required.add_argument('-d', '--db-prefix',         required=True,  type=str,               metavar='db_prefix',        help='Database prefix')
    update_group_required.add_argument('-i', '--input-files',       required=True,  type=str, nargs="*",    metavar='refs.fasta[.gz]',  help='Multi-fasta[.gz] file[s]')
    
    # Defaults
    update_group_optional = update_parser.add_argument_group('optional arguments')
    update_group_optional.add_argument('-o', '--output-db-prefix',                  type=str,               metavar='', help='Alternative output database prefix. Default: overwrite current --db-prefix')
    #update_group_optional.add_argument('-c', '--update-complete',                             default=False, action='store_true', help='Update complete bins, removing sequences. Input file should be complete, not only new sequences.')
    update_group_optional.add_argument('-t', '--threads',                           type=int, default=2,    metavar='', help='set the number of subprocesses/threads to use for calculations. Default: 2')
    update_group_optional.add_argument('--len-taxid-file',                          type=str,               metavar='', help='Tab-separated file with the sequence ids, sequence length, taxonomic id and optionally the assembly accession in case of assembly level classification is chosen')
    update_group_optional.add_argument('--nodes-file',                              type=str,               metavar='', help='nodes.dmp from NCBI Taxonomy')
    update_group_optional.add_argument('--merged-file',                             type=str,               metavar='', help='merged.dmp from NCBI Taxonomy')
    update_group_optional.add_argument('--names-file',                              type=str,               metavar='', help='names.dmp from NCBI Taxonomy')
    # Extra
    update_group_optional.add_argument('--verbose',                                           default=False, action='store_true', help='Verbose mode for ganon')
    update_group_optional.add_argument('--ganon-path', type=str, default="./", help=argparse.SUPPRESS)
    update_group_optional.add_argument('--taxsbp-path', type=str, default="./", help=argparse.SUPPRESS)
    update_group_optional.add_argument('--n-refs', type=int, help=argparse.SUPPRESS)
    update_group_optional.add_argument('--n-batches', type=int, help=argparse.SUPPRESS)

    ####################################################################################################

    classify_parser = argparse.ArgumentParser(description='Classification options', add_help=False)
    
    # Required
    classify_group_required = classify_parser.add_argument_group('required arguments')
    classify_group_required.add_argument('-d', '--db-prefix',    required=True, type=str,              nargs="*", metavar='db_prefix', help='Database prefix[es]')
    classify_group_required.add_argument('-r', '--reads',        required=True, type=str,              nargs="*", metavar='reads.fq[.gz]', help='Multi-fastq[.gz] file[s] to classify')

    # Defaults
    classify_group_optional = classify_parser.add_argument_group('optional arguments')
    classify_group_optional.add_argument('-c', '--db-hierarchy',                type=str, default=["1"], nargs="*", metavar='int', help='Hierachy definition, one for each database input. Can also be string, but input will be sorted (e.g. 1 1 2 3). Default: 1')
    classify_group_optional.add_argument('-e', '--max-error',                   type=int, default=[3],   nargs="*", metavar='int', help='Max. number of errors allowed. Single value or one per database (e.g. 3 3 4 0). Default: 3')
    classify_group_optional.add_argument('-u', '--max-error-unique',            type=int, default=[-1],  nargs="*", metavar='int', help='Max. number of errors allowed for unique assignments after filtering. Matches below this error rate will not be discarded, but assigned to parent taxonomic level. Single value or one per hierachy (e.g. 0 1 2). -1 to disable. Default: -1')
    classify_group_optional.add_argument('-f', '--offset',                      type=int, default=1,              metavar='', help='Number of k-mers to skip during clasification. Can speed up analysis but may reduce recall. (e.g. 1 = all k-mers, 3 = every 3rd k-mer). Function must be enabled on compilation time with -DGANON_OFFSET=ON. Default: 1')
    classify_group_optional.add_argument('-o', '--output-file-prefix',          type=str, default="",             metavar='', help='Output file name prefix. .out for complete results and .lca for LCA results. Empty to print to STDOUT (only with lca). Default: ""')
    classify_group_optional.add_argument('-n', '--output-unclassified-file',    type=str, default="",             metavar='', help='Output file for unclassified reads headers. Empty to not output. Default: ""')
    classify_group_optional.add_argument('-s', '--split-output-file-hierarchy', default=False, action='store_true',               help='Split output in multiple files by hierarchy. Appends "_hierachy" to the --output-file definiton.')
    classify_group_optional.add_argument('-l', '--skip-lca',                    default=False, action='store_true',               help='Skip LCA step and output multiple matches. --max-error-unique will not be applied')
    classify_group_optional.add_argument('-p', '--report',                      default=False, action='store_true',               help='Generate reports')
    classify_group_optional.add_argument('-t', '--threads',                     type=int, default=3,              metavar='', help='Number of subprocesses/threads. Default: 3)')
    # Extra
    classify_group_optional.add_argument('--verbose',                           default=False, action='store_true',  help='Output in verbose mode for ganon-classify')
    classify_group_optional.add_argument('--ganon-path', type=str, default="./", help=argparse.SUPPRESS)
    classify_group_optional.add_argument('--n-reads', type=int, help=argparse.SUPPRESS)
    classify_group_optional.add_argument('--n-batches', type=int, help=argparse.SUPPRESS)

    ####################################################################################################

    parser = argparse.ArgumentParser(prog='ganon', description='ganon', conflict_handler='resolve')
    parser.add_argument('-v', '--version', action='version', version='%(prog)s ' + version, help="Show program's version number and exit.")
    subparsers = parser.add_subparsers()
    
    build = subparsers.add_parser('build', help='Build ganon database', parents=[build_parser])
    build.set_defaults(which='build')

    update = subparsers.add_parser('update', help='Update ganon database', parents=[update_parser])
    update.set_defaults(which='update')

    classify = subparsers.add_parser('classify', help='Classify reads', parents=[classify_parser])
    classify.set_defaults(which='classify')

    args = parser.parse_args()

    if len(sys.argv[1:])==0: # Print help calling script without parameters
        parser.print_help() 
        return 0
    
    tx_total = time.time()

    # Validations
    if not os.path.isfile(args.ganon_path + "/ganon-classify") or not os.path.isfile(args.ganon_path + "/ganon-build"):
        sys.stderr.write("ganon binaries (ganon-classify and ganon-build) were not found. Please inform the path of the binaries with --ganon-path\n")
        return 1

    if args.which=='build' or args.which=='update':
        db_prefix = args.db_prefix
        output_folder = os.path.abspath(os.path.dirname(db_prefix)) + "/"
        tmp_output_folder = db_prefix + "_tmp/"
        db_prefix_filter = db_prefix + ".filter"
        db_prefix_nodes = db_prefix + ".nodes"
        db_prefix_map = db_prefix + ".map"
        db_prefix_bins = db_prefix + ".bins"
        
        if not os.path.isfile(args.taxsbp_path + "/TaxSBP.py") or not os.path.isfile(args.taxsbp_path + "/scripts/get_len_taxid.sh"):
            sys.stderr.write("TaxSBP files (TaxSBP.py and scripts/get_len_taxid.sh) were not found. Please inform the path of the files with --taxsbp-path\n")
            return 1

    if args.which=='build':
        # if -1 set default, if not set value (0 deactivated)
        if args.fragment_length==-1:
            fragment_length = args.bin_length - args.overlap_length
        elif args.fragment_length==0:
            fragment_length = 0
        else:
            fragment_length = args.fragment_length - args.overlap_length 

        if fragment_length and args.overlap_length > fragment_length:
            sys.stderr.write("--overlap_length cannot be bigger than --fragment_length\n")
            return 1

    if args.which=='classify':
        if args.skip_lca and not args.output_file_prefix:
            sys.stderr.write("--output-file-prefix is mandatory without LCA\n")
            return 1

    if args.which=='build':     
        use_assembly=True if args.rank=="assembly" else False
        taxsbp_input_file, ncbi_nodes_file, ncbi_merged_file, ncbi_names_file = prepare_taxsbp_files(args, tmp_output_folder, use_assembly)

        tx = time.time()
        sys.stderr.write("Running taxonomic clustering (TaxSBP.py)... \n")
        run_taxsbp_cmd = 'python3 {0}TaxSBP.py cluster -f {1} -n {2} {3} -l {4} -r {5} {6} {7} {8}'.format(
                            args.taxsbp_path,
                            taxsbp_input_file,
                            ncbi_nodes_file,
                            "-m " + ncbi_merged_file if ncbi_merged_file else "",
                            args.bin_length,
                            "assembly" if use_assembly else args.rank,
                            "-z assembly" if use_assembly else "",
                            "-a " + str(fragment_length) if fragment_length else "",
                            "-o " + str(args.overlap_length) if fragment_length else "")
        stdout, stderr, errcode = run(run_taxsbp_cmd, print_stderr=True)
        acc_bin_file = tmp_output_folder + "acc_bin.txt"
        actual_number_of_bins, unique_taxids, max_length_bin = taxsbp_output_files(stdout, acc_bin_file, db_prefix_bins, db_prefix_map, use_assembly, fragment_length)
        sys.stderr.write(str(actual_number_of_bins) + " bins created. ")
        sys.stderr.write("Done. Elapsed time: " + str(time.time() - tx) + " seconds.\n")

        # aproximate number of unique k-mers by just considering that they are all unique
        max_kmer_count = max_length_bin-args.kmer_size+1
        sys.stderr.write("Approximate (upper-bound) # unique k-mers: " + str(max_kmer_count) + "\n")
        # get optimal number of bins to get correct fp rate based on IBF implementation (always next multiple of 64)
        optimal_number_of_bins = math.ceil(actual_number_of_bins/64)*64

        # define bloom filter size based on given false positive
        MBinBits = 8388608
        if not args.bloom_size:
            bin_size_bits = math.ceil(-(1/((1-args.max_fp**(1/float(args.hash_functions)))**(1/float(args.hash_functions*max_kmer_count))-1)))   
            sys.stderr.write("Bloom filter calculated size with fp<=" + str(args.max_fp) + ": " + str("{0:.4f}".format((bin_size_bits*optimal_number_of_bins)/MBinBits)) + "MB (" + str(bin_size_bits) + " bits/bin * " + str(optimal_number_of_bins) + " optimal bins [" + str(actual_number_of_bins) + " real bins])\n")
        else:
            bin_size_bits = math.ceil((args.bloom_size * MBinBits)/optimal_number_of_bins);
            estimated_max_fp = (1-((1-(1/float(bin_size_bits)))**(args.hash_functions*max_kmer_count)))**args.hash_functions
            sys.stderr.write("Bloom filter calculated max. fp with size=" + str(args.bloom_size) + "MB: " + str("{0:.4f}".format(estimated_max_fp) + " ("  + str(optimal_number_of_bins) + " optimal bins [" + str(actual_number_of_bins) + " real bins])\n"))

        tx = time.time()
        sys.stderr.write("Building index (ganon-build)... \n")
        run_ganon_build_cmd = '{0}ganon-build -e {1} {2} {3} -k {4} -n {5} -t {6} -o {7} {8} {9} {10} {11}'.format(
                                        args.ganon_path,
                                        acc_bin_file,
                                        "--filter-size-bits" if args.max_fp else "--filter-size",
                                        bin_size_bits*optimal_number_of_bins if args.max_fp else args.bloom_size,
                                        args.kmer_size,
                                        args.hash_functions,
                                        args.threads,
                                        db_prefix_filter,
                                        "--verbose" if args.verbose else "",
                                        "--n-refs " + str(args.n_refs) if args.n_refs is not None else "",
                                        "--n-batches " + str(args.n_batches) if args.n_batches is not None else "",
                                        " ".join([file for file in args.input_files]))
        stdout, stderr, errcode = run(run_ganon_build_cmd, print_stderr=True)
        sys.stderr.write("Done. Elapsed time: " + str(time.time() - tx) + " seconds.\n")

        tx = time.time()
        sys.stderr.write("Storing nodes... ")
        info = {'kmer_size':args.kmer_size, 
                'hash_functions': args.hash_functions, 
                'number_of_bins': actual_number_of_bins, 
                'rank': args.rank,
                'bin_length': args.bin_length,
                'fragment_length': fragment_length,
                'overlap_length': args.overlap_length,
                'filtered_nodes': {},
                'filtered_names': {}}
        nodes = read_nodes(ncbi_nodes_file)
        names = read_names(ncbi_names_file)
    
        # Filter nodes for used taxids
        for leaf_taxid in unique_taxids:
            t = leaf_taxid
            while t!=0:
                info['filtered_nodes'][t] = nodes[t]
                info['filtered_names'][t] = names[t]
                t = nodes[t]
                if t in info['filtered_nodes']: break # branch already in the dict

        with open(db_prefix_nodes, 'wb') as f: pickle.dump(info, f)

        # Delete temp files
        shutil.rmtree(tmp_output_folder)

        sys.stderr.write("Done. Elapsed time: " + str(time.time() - tx) + " seconds.\n")
        
    elif args.which=='update':  
        tx = time.time()

        tmp_db_prefix = tmp_output_folder + "tmp"
        tmp_db_prefix_filter = tmp_db_prefix + ".filter"
        tmp_db_prefix_nodes = tmp_db_prefix + ".nodes"
        tmp_db_prefix_map = tmp_db_prefix + ".map"
        tmp_db_prefix_bins = tmp_db_prefix + ".bins"

        kmer_size, hash_functions, number_of_bins, rank, bin_length, fragment_length, overlap_length, filtered_nodes, filtered_names = parse_db_prefix_nodes(db_prefix_nodes)
        use_assembly=True if rank=="assembly" else False

        taxsbp_input_file, ncbi_nodes_file, ncbi_merged_file, ncbi_names_file = prepare_taxsbp_files(args, tmp_output_folder, use_assembly)

        sys.stderr.write("Running taxonomic clustering (TaxSBP.py)... \n")
        run_taxsbp_cmd = 'python3 {0}TaxSBP.py cluster -u {1} -f {2} -n {3} {4} -l {5} -r {6} {7} {8} {9}'.format(
                            args.taxsbp_path,
                            db_prefix_bins,
                            taxsbp_input_file,
                            ncbi_nodes_file,
                            "-m " + ncbi_merged_file if ncbi_merged_file else "",
                            bin_length,
                            "assembly" if use_assembly else rank,
                            "-z assembly" if use_assembly else "",
                            "-a " + str(fragment_length) if fragment_length else "",
                            "-o " + str(overlap_length) if fragment_length else "")
        stdout, stderr, errcode = run(run_taxsbp_cmd, print_stderr=True)
    
        acc_bin_file = tmp_output_folder + "acc_bin.txt"
        updated_bins, unique_taxids, _ = taxsbp_output_files(stdout, acc_bin_file, tmp_db_prefix_bins, tmp_db_prefix_map, use_assembly, fragment_length)
        sys.stderr.write(str(updated_bins) + " bins updated. ")
        sys.stderr.write("Done. Elapsed time: " + str(time.time() - tx) + " seconds.\n")

        tx = time.time()
        sys.stderr.write("Updating index (ganon-build)... \n")
        run_ganon_build_cmd = '{0}ganon-build -u {1} -e {2} -o {3} -t {4} {5} {6} {7} {8}'.format(
                                        args.ganon_path,
                                        db_prefix_filter,
                                        acc_bin_file,
                                        tmp_db_prefix_filter,
                                        args.threads,
                                        "--verbose" if args.verbose else "",
                                        "--n-refs " + str(args.n_refs) if args.n_refs is not None else "",
                                        "--n-batches " + str(args.n_batches) if args.n_batches is not None else "",
                                        " ".join([file for file in args.input_files]))
        stdout, stderr, errcode = run(run_ganon_build_cmd, print_stderr=True)
        sys.stderr.write("Done. Elapsed time: " + str(time.time() - tx) + " seconds.\n")

        tx = time.time()
        sys.stderr.write("Storing nodes... ")
        info = {'kmer_size': kmer_size, 
                'hash_functions': hash_functions, 
                'number_of_bins': number_of_bins, 
                'rank': rank,
                'bin_length': bin_length,
                'fragment_length': fragment_length,
                'overlap_length': overlap_length,
                'filtered_nodes': {},
                'filtered_names': {}}

        nodes = read_nodes(ncbi_nodes_file)
        names = read_names(ncbi_names_file)

        # Filter nodes for used taxids
        for leaf_taxid in unique_taxids:
            t = leaf_taxid
            while t!=0:
                info['filtered_nodes'][t] = nodes[t]
                info['filtered_names'][t] = names[t]
                t = nodes[t]
                if t in info['filtered_nodes']: break # branch already in the dict

        # Merge nodes, duplicates solved by new nodes
        info['filtered_nodes'].update(filtered_nodes)
        info['filtered_names'].update(filtered_names)

        with open(tmp_db_prefix_nodes, 'wb') as f: pickle.dump(info, f)
        sys.stderr.write("Done. Elapsed time: " + str(time.time() - tx) + " seconds.\n")

        # Set new files
        if args.output_db_prefix:
            db_prefix_filter = args.output_db_prefix + ".filter"
            db_prefix_nodes = args.output_db_prefix + ".nodes"
            
            shutil.copyfile(db_prefix_map, args.output_db_prefix + ".map")
            db_prefix_map = args.output_db_prefix + ".map"

            shutil.copyfile(db_prefix_bins, args.output_db_prefix + ".bins")
            db_prefix_bins = args.output_db_prefix + ".bins"


        shutil.move(tmp_db_prefix_filter, db_prefix_filter)
        shutil.move(tmp_db_prefix_nodes, db_prefix_nodes)
        
        # append new map
        old_map_file = open(db_prefix_map, "a")
        with open(tmp_db_prefix_map, 'r') as file:
            for line in file:
                old_map_file.write(line) # TODO -> remove duplicated lines
        old_map_file.close()

        
        # append new bins
        old_bins_file = open(db_prefix_bins, "a")
        with open(tmp_db_prefix_bins, 'r') as file:
            for line in file:
                old_bins_file.write(line)
        old_bins_file.close()

        # Delete temp files
        shutil.rmtree(tmp_output_folder)

    elif args.which=='classify':

        ganon_classify_output_file = "ganon-classify-tmp.out" if not args.output_file_prefix else args.output_file_prefix+".out"
        tx = time.time()
        sys.stderr.write("Classifying reads (ganon-classify)... \n")
        run_ganon_classify = '{0}ganon-classify {1} {2} -c {3} -e {4} -u {5} -t {6} -f {7} -o {8} {9} {10} {11} {12} {13} {14}'.format(
                                        args.ganon_path,
                                        " ".join(["-b "+db_prefix+".filter" for db_prefix in args.db_prefix]),
                                        " ".join(["-g "+db_prefix+".map" for db_prefix in args.db_prefix]),
                                        ",".join([str(h) for h in args.db_hierarchy]),
                                        ",".join([str(me) for me in args.max_error]),
                                        ",".join([str(meu) for meu in args.max_error_unique]),
                                        args.threads,
                                        args.offset,
                                        ganon_classify_output_file,
                                        "-s" if args.split_output_file_hierarchy else "",
                                        "-n " + args.output_unclassified_file if args.output_unclassified_file else "",
                                        "--verbose" if args.verbose else "",
                                        "--n-reads " + str(args.n_reads) if args.n_reads is not None else "",
                                        "--n-batches " + str(args.n_batches) if args.n_batches is not None else "",
                                        " ".join(args.reads))
        stdout, stderr, errcode = run(run_ganon_classify, print_stderr=True)
        sys.stderr.write("\nDone. Elapsed time: " + str(time.time() - tx) + " seconds.\n")

        if not args.skip_lca:
            from scripts.LCA import LCA

            tx = time.time()
            sys.stderr.write("Generating LCA results... ")

            report = {}

            # build hierarchy structure for output lca files
            output_hierarchy = defaultdict(list)
            if len(args.db_hierarchy) > 1 and args.split_output_file_hierarchy:
                for dbid,dbp in enumerate(args.db_prefix):
                    output_hierarchy[args.db_hierarchy[dbid]].append(dbp)
            else:
                output_hierarchy[None] = args.db_prefix # no slit or no hierarchy, output together
 
            for hierarchy_name, db_prefixes in output_hierarchy.items():

                # check if prefix for multiple files is necessary
                h_prefix = "_"+hierarchy_name if hierarchy_name is not None else ""
                
                # Merge group-taxid information and nodes from multiple databases
                merged_group_taxid = {}
                merged_filtered_nodes = {}
                merged_filtered_names = {}
                for db_prefix in db_prefixes:
                    _, _, _, rank, _, _, _, filtered_nodes, filtered_names = parse_db_prefix_nodes(db_prefix+".nodes")
                    use_assembly=True if rank=="assembly" else False # if one of them uses assembly should be True
                    merged_group_taxid.update(parse_db_prefix_bins(db_prefix+".bins", use_assembly))
                    merged_filtered_nodes.update(filtered_nodes)
                    merged_filtered_names.update(filtered_names)
                    
                # pre build LCA with used nodes
                L = LCA(merged_filtered_nodes)

                # redirect output for lca
                if args.output_file_prefix: sys.stdout = open(args.output_file_prefix+".lca"+h_prefix,'w')


                report[hierarchy_name] = defaultdict(int)
                with open(ganon_classify_output_file+h_prefix) as file:
                    try: # read first entry
                        old_readid, cl, kc = file.readline().rstrip().split("\t")
                        assignments = set([cl])
                        max_kmer_count = int(kc)
                        done = False
                    except (ValueError, IndexError): # last line -> empty, no reads classified
                        done=True # do not start

                    while not done:
                        try:
                            readid, cl, kc = file.readline().rstrip().split("\t")
                        except (ValueError, IndexError): # last line -> empty
                            readid="" # clear variable to print last entry on this iteration
                            done=True # exit on next iteration
                            
                        # next read matches, print old
                        if readid != old_readid: 
                            taxid_lca = get_lca_read(assignments, max_kmer_count, merged_filtered_nodes, L, merged_group_taxid, use_assembly)
                            print(old_readid, taxid_lca, max_kmer_count, sep="\t")
                            if args.report: report[hierarchy_name][taxid_lca] += 1
                            assignments.clear()
                            max_kmer_count=0
                    
                        if not done: #if not last line
                            assignments.add(cl)
                            kc = int(kc)
                            max_kmer_count = kc if abs(kc) > max_kmer_count else max_kmer_count # account for unique filtering with abs
                            old_readid = readid

                # Rm file if tmp
                if not args.output_file_prefix: 
                	os.remove(ganon_classify_output_file+h_prefix) 
               	else: # close open file if not
               	    sys.stdout.close()

            print(report)
                    
            sys.stderr.write("Done. Elapsed time: " + str(time.time() - tx) + " seconds.\n")

    sys.stderr.write("Total elapsed time: " + str(time.time() - tx_total) + " seconds.\n")
   
def get_lca_read(assignments, max_kmer_count, merged_filtered_nodes, L, merged_group_taxid, use_assembly):
    if len(assignments)==1: # unique match or same taxid (but one or more assemblies)  
        if max_kmer_count<0: # unique matches in ganon-classify, get leaf taxid (assembly) or parent taxid
            return merged_group_taxid[assignments.pop()] if use_assembly else merged_filtered_nodes[int(assignments.pop())]
        else:
            return assignments.pop()
    else:
        # Recover taxids from assignments (assembly) or convert them to int (taxid) on the lookup
        assignments = set(map(lambda x: merged_group_taxid[x], assignments)) 
        if use_assembly and len(assignments)==1: # If all assignments are on the same taxid, no need to lca and return it
            return assignments.pop()

        taxid_lca = L(assignments.pop(),assignments.pop())
        for i in range(len(assignments)): 
            taxid_lca = L(taxid_lca, assignments.pop())
            if merged_filtered_nodes[taxid_lca] == 0: break #if lca is already root node, no point in continue

        return taxid_lca

def prepare_taxsbp_files(args, tmp_output_folder, use_assembly):
    # Create temporary working directory
    if os.path.exists(tmp_output_folder): shutil.rmtree(tmp_output_folder) # delete if already exists
    os.makedirs(tmp_output_folder)
    
    # Prepare TaxSBP files
    if not args.len_taxid_file:
        taxsbp_input_file = retrieve_ncbi(tmp_output_folder, args.input_files, args.threads, args.taxsbp_path, use_assembly)
    else:
        taxsbp_input_file = args.len_taxid_file

    if not args.nodes_file:
        ncbi_nodes_file, ncbi_merged_file, ncbi_names_file = get_taxdump(tmp_output_folder)
    else:
        ncbi_nodes_file = args.nodes_file 
        ncbi_merged_file = args.merged_file
        ncbi_names_file = args.names_file

    return taxsbp_input_file, ncbi_nodes_file, ncbi_merged_file, ncbi_names_file

def run(cmd, output_file=None, print_stderr=False):
    try:
        errcode=0
        process = subprocess.Popen(shlex.split(cmd), shell=False, universal_newlines=True, stdout=open(output_file, 'w') if output_file else subprocess.PIPE, stderr=subprocess.PIPE)   
        stdout, stderr = process.communicate() # wait for the process to terminate
        errcode = process.returncode
        if errcode!=0: raise Exception()
        if print_stderr: sys.stderr.write(stderr)
        return stdout, stderr, errcode
    #except OSError as e: # The most common exception raised is OSError. This occurs, for example, when trying to execute a non-existent file. Applications should prepare for OSError exceptions.
    #except ValueError as e: #A ValueError will be raised if Popen is called with invalid arguments.
    except Exception as e:
        sys.stderr.write('The following command failed to execute:\n'+cmd)
        sys.stderr.write(str(e)+"\n")
        sys.stderr.write("Errorcode: "+str(errcode)+"\n")
        sys.stderr.write("Error: "+stderr+"\n")
        raise

def taxsbp_output_files(stdout, acc_bin_file, db_prefix_bins, db_prefix_map, use_assembly, fragment_length):
    bins = open(db_prefix_bins,'w')
    acc_bin = open(acc_bin_file,'w') #.temp for build
    group_bin = open(db_prefix_map,'w') #.map
    unique_taxids = set() # for nodes
    bin_length = defaultdict(int) # calculate max bin length

    for line in stdout.split("\n"):
        if line:
            #acc, length, taxid, [group/rank taxid,] binno
            if use_assembly:
                acc, length, taxid, group, binno = line.split("\t")
            else:
                acc, length, taxid, binno = line.split("\t")
                group = taxid # taxid is the classification group

            # if sequences are fragmentes
            if fragment_length: 
                acc, coord = acc.split("/")
                frag_start, frag_end = coord.split(":")
            else:
                frag_start = 1
                frag_end = length

            bin_length[binno]+=int(length)
            unique_taxids.add(int(taxid))
            print(line, file=bins) 
            print(acc, frag_start, frag_end, binno, sep="\t", file=acc_bin)
            print(group, binno, sep="\t", file=group_bin)
    bins.close()
    acc_bin.close()
    group_bin.close()
    return len(bin_length), unique_taxids, max(bin_length.values())

def get_taxdump(tmp_output_folder):
    tx = time.time()
    sys.stderr.write("Downloading taxdump... ")
    taxdump_file = tmp_output_folder+'taxdump.tar.gz'
    run_wget_taxdump_cmd = 'wget -qO {0} ftp://ftp.ncbi.nih.gov/pub/taxonomy/taxdump.tar.gz'.format(taxdump_file)
    stdout, stderr, errcode = run(run_wget_taxdump_cmd, print_stderr=True)
    unpack_taxdump_cmd = 'tar xf {0} -C "{1}" nodes.dmp merged.dmp names.dmp'.format(taxdump_file, tmp_output_folder)
    stdout, stderr, errcode = run(unpack_taxdump_cmd, print_stderr=True)
    sys.stderr.write("Done. Elapsed time: " + str(time.time() - tx) + " seconds.\n")
    return tmp_output_folder+'nodes.dmp', tmp_output_folder+'merged.dmp', tmp_output_folder+'names.dmp'

def retrieve_ncbi(tmp_output_folder, files, threads, taxsbp_path, use_assembly):
    tx = time.time()
    accessions_file = tmp_output_folder + 'accessions.txt'
    sys.stderr.write("Extracting accessions... ")
    run_get_header = 'zgrep -hoP "(?<=^>)[^ ]+" {0}'.format(" ".join(files))
    stdout, stderr, errcode = run(run_get_header, output_file=accessions_file, print_stderr=False)
    sys.stderr.write("Done. Elapsed time: " + str(time.time() - tx) + " seconds.\n")

    tx = time.time()
    sys.stderr.write("Retrieving data from NCBI... ")
    taxsbp_input_file = tmp_output_folder + 'acc_len_taxid.txt'
    run_get_len_taxid_cmd = '{0}get_len_taxid.sh -i {1} {2}'.format(
                            taxsbp_path+"/scripts/",
                            accessions_file,
                            "-a" if use_assembly else "")
    _, stderr, errcode = run(run_get_len_taxid_cmd, output_file=taxsbp_input_file, print_stderr=True)
    sys.stderr.write("Done. Elapsed time: " + str(time.time() - tx) + " seconds.\n")
    return taxsbp_input_file

def read_nodes(nodes_file):
    # READ nodes -> fields (1:TAXID 2:PARENT_TAXID)
    nodes = {}
    with open(nodes_file,'r') as fnodes:
        for line in fnodes:
            taxid, parent_taxid, _, _ = line.split('\t|\t',3)
            nodes[int(taxid)] = int(parent_taxid)
    nodes[1] = 0 
    return nodes

def read_names(names_file):
    # READ names -> fields (1:TAXID 2:NAME 3:UNIQUE NAME 4:NAME CLASS)
    names = {}
    with open(names_file,'r') as fnames:
        for line in fnames:
            taxid, name, _, name_class = line.split('\t|\t')
            if name_class.replace('\t|\n','')=="scientific name":
                names[int(taxid)] = name
    return names

def parse_db_prefix_bins(file, use_assembly):
    groupTaxid = {}
    with open(file,'r') as bins:
        for line in bins:
            if use_assembly:
                acc, length, taxid, group, binno = line.split("\t")
            else:
                acc, length, taxid, binno = line.split("\t")
                group = taxid # taxid is the classification group
            groupTaxid[group] = int(taxid) # this is done in case multiple databases are used, some with assembly others without
    return groupTaxid

def parse_db_prefix_nodes(file):
    info = pickle.load(open(file, "rb"))
    kmer_size = info['kmer_size']
    hash_functions = info['hash_functions']
    number_of_bins = info['number_of_bins']
    rank = info['rank']
    bin_length = info['bin_length']
    fragment_length = info['fragment_length']
    overlap_length = info['overlap_length']
    filtered_nodes = info['filtered_nodes']
    filtered_names = info['filtered_names'] if 'filtered_names' in info else {}
    return kmer_size, hash_functions, number_of_bins, rank, bin_length, fragment_length, overlap_length, filtered_nodes, filtered_names

if __name__ == '__main__':
    main()
